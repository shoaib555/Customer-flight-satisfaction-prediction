
## Flight Customer Satisfaction Prediction
```{r}
#loading the datasets and merging it on ID and basic EDA
rm(list=ls())
su=read.csv("Survey_data.csv",na.strings = c("",NA,NaN))
fl=read.csv("Flight+data.csv",na.strings = c("",NA,NaN))
summary(fl) 
summary(su)
colnames(fl)[1]="Id"
ff=merge(x = su, y = fl, by = "Id", all.x = TRUE)
str(ff)
colSums(is.na(ff))
library(DataExplorer)
plot_missing(ff)

#dropping rows that have more than 1 missing variables
delete.na <- function(DF, n=0) {
  DF[rowSums(is.na(DF)) <= n,]
}
f1=delete.na(ff,1)
sapply(f1, function(x) sum(is.na(x)))
dim(f1)

table(f1$Gate_location)
table(f1$Baggage_handling)
table(f1$Seat_comfort)
l1=f1[,c(2,3,4,5,7:16)]

#collapsing the 5 factors to 4
f1$Gate_location[f1$Gate_location=="very inconvinient"]="Inconvinient"
```

## Visualization
```{r}
l2=na.omit(l1)
library(reshape2)
st1=melt(l2, id = c("Satisfaction"))

#  bar plots
library(tidyverse)
st1%>%ggplot(aes(x=value))+geom_bar(position = position_dodge(),aes(fill=Satisfaction), alpha=0.7 ) +
  facet_wrap(.~variable,scales = "free_x", nrow = 3)+ggtitle("Bar Plots for Catigorical variables vs DV")+coord_flip()+
  scale_fill_manual(values=c('red','green'))+guides(fill=F)

f1%>%ggplot(aes(Gate_location,fill=Satisfaction))+geom_bar(position=position_dodge())+scale_fill_manual(values=c('red','green'))+guides(fill=F)+ggtitle("Gate Location vs DV")

f1%>%ggplot(aes(Gender,fill=Satisfaction))+geom_bar(position=position_dodge())+scale_fill_manual(values=c('red','green'))+guides(fill=F)+ggtitle("Gender vs DV")

f1%>%ggplot(aes(CustomerType,fill=Satisfaction))+geom_bar(position=position_dodge())+scale_fill_manual(values=c('red','green'))+guides(fill=F)+ggtitle("Customer Type vs DV")


f1%>%ggplot(aes(TypeTravel,fill=Satisfaction))+geom_bar(position=position_dodge())+scale_fill_manual(values=c('red','green'))+guides(fill=F)+ggtitle("Travel Type vs DV")

f1%>%ggplot(aes(Class,fill=Satisfaction))+geom_bar(position=position_dodge())+scale_fill_manual(values=c('red','green'))+guides(fill=F)+ggtitle("Class vs DV")

summary(f1$Age)
f1$Age=as.numeric(f1$Age)
str(f1$Age)
f1$Age_bin=ifelse(f1$Age ==7 & f1$Age <=27,"Young",ifelse(f1$Age>27 & f1$Age <=40,"middle Age",ifelse(f1$Age>40 & f1$Age <=51,"Old","Very Old")))

f1%>%ggplot(aes(Age_bin,fill=Satisfaction))+geom_bar(position=position_dodge())+scale_fill_manual(values=c('red','green'))+guides(fill=F)+ggtitle("Age_bin vs DV")

table(f1$Age_bin)

f1%>%group_by(Satisfaction)%>%dplyr::summarize(Avg_Age=mean(Age),Avg_dist=mean(Flight_Distance),Avg_dep_delay=mean(DepartureDelayin_Mins),
                                               Avg_arr_delay=mean(ArrivalDelayin_Mins,na.rm = T))


```

## Label ecoding the feedback columns
```{r}
library(plyr)
l1=f1[,c(1,3:5,7:16)]

colSums(is.na(l1))

ll= reshape2::dcast(dplyr::mutate(reshape2::melt(l1,id.var="Id"),
    value=plyr::mapvalues(value,c("acceptable","excellent","extremely poor","good","need improvement","poor",NA),
    c(3,5,0,4,2,1,NA))),Id~variable)

ll=lapply(ll, function(x) as.integer(x))
ll=as.data.frame(ll)

f1$Gate_location=ifelse(f1$Gate_location=="Inconvinient",0,ifelse(f1$Gate_location=="need improvement",1,
                  ifelse(f1$Gate_location=="manageable",2,ifelse(f1$Gate_location=="Convinient",2,4))))

f1=f1[,-c(1,3:5,7:16)]
ff=cbind(ll,f1)
str(ff)
```

```{r}
#percentage of customers who are satisfied given inflight entertainment is below 3
ff%>%filter(Inflight_entertainment>3 & Satisfaction!="satisfied")%>%dplyr::summarize(percent=round(n()/length(ff$Satisfaction),4)*100)

#percentage of customers who are satisfied given inflight entertainment is above 3
ff%>%filter(Inflight_entertainment>3 & Satisfaction=="satisfied")%>%dplyr::summarize(percent=round(n()/length(ff$Satisfaction),4)*100)

#average rating for online related feedbacks and their counts
ff%>%select(Ease_of_Onlinebooking,Online_boarding,Online_support)%>%mutate(avg_online_rating=rowMeans(.[,1:3]))%>%group_by(avg_online_rating)%>%dplyr::summarize(count=n())%>%arrange(desc(avg_online_rating))

```


```{r}
# label encoding
ff$Gender=ifelse(ff$Gender=="Male",1,0)
ff$Class=ifelse(ff$Class=="Business",2,ifelse(ff$Class=="Eco Plus",1,0))
ff$CustomerType=ifelse(ff$CustomerType=="Loyal Customer",1,0)
ff$Satisfaction=ifelse(ff$Satisfaction=="satisfied",1,0)
ff$TypeTravel=ifelse(ff$TypeTravel=="Personal Travel",0,1) 
```

```{r}
#missing value treatment with mice
ff=ff[,-1]

library(mice)
md.pattern(ff)
mymice=mice(ff,m=2 ,method="pmm")
mymiceComplete=complete(mymice,2)
summary(mymiceComplete)
ff=mymiceComplete
summary(ff)
ff=ff[,-24]
```


## PCA
```{r}
summary(ff)
pc=ff[,c(1:13,15)]
library(psych)
cortest.bartlett(pc)
KMO(pc)
pc=scale(pc)

cormatrix=cor(pc)
A=eigen(cormatrix)
ev=A$values
ev

#choosing factors
library(nFactors)
####scree plot
plot(ev,xlab="Factors",ylab="Eigen Value",pch=20,col="blue")+lines(ev,col="red")
eFactors=fa(pc,nfactors = 3,rotate = "varimax",fm="wls")
eFactors
fa.diagram(eFactors)
attributes(eFactors)

pc1=cbind(pc,Digital_score=eFactors$scores[,1],In_flight_score=eFactors$scores[,3],comfort_score=eFactors$scores[,2])
pc1=as.data.frame(pc1)
pc1=pc1[,-c(1:10,12:14)]

pc1=cbind(ff$Satisfaction,pc1)
colnames(pc1)[1]="Satisfaction"

pc1=cbind(pc1,ff[,c(16:23)])
sc=scale(pc1[,-c(1,3:7,9,10)])
sc=as.data.frame(sc)

summary(sc)
sc=cbind(pc1[,c(1,3:7,9,10)],sc)
sc$Satisfaction=as.factor(sc$Satisfaction)
str(sc)
write.csv(sc,"C:\\Users\\Shoaib\\Videos\\R VS PY\\sc.csv",row.names = F)
```

## Logistic Regression
```{r}
set.seed(123)
library(caTools)
spl=sample.split(sc$Satisfaction,SplitRatio = 0.7)
tr=subset(sc,spl==T)
ts=subset(sc,spl==F)

library(caret)
set.seed(1235)
lg=glm(Satisfaction~.,data=tr,family = binomial(link="logit"))
summary(lg)
ts$prob=predict(lg,ts,type="response")
ts$pred=ifelse(ts$prob>0.5,1,0)
ts$pred=as.factor(ts$pred)
ts$Satisfaction=as.factor(ts$Satisfaction)
confusionMatrix(ts$Satisfaction,ts$pred,positive = "1")

library(car)
vif(lg)

sc=sc[,-12]

str(sc)
```



## Knn
```{r}
set.seed(400)
ctrl=trainControl(method="cv",number=10)
knn=train(Satisfaction ~ ., data = tr, method = "knn", trControl = ctrl,tuneGrid = expand.grid(k = c(3,5,7,9,11)))
knn
plot(knn)
ts$prob=predict(knn,ts,type="prob")[,'1']
ts$pred=ifelse(ts$prob>0.5,1,0)
ts$pred=as.factor(ts$pred)
ts$Satisfaction=as.factor(ts$Satisfaction)
confusionMatrix(ts$Satisfaction,ts$pred,positive = "1")

varImp(knn)
```


## CART
```{r}

library(rpart)
library(rpart.plot)
library(rattle)
set.seed(897)
r.ctrl=rpart.control(minisplit=10,minbucket=5,cp=0,xval=10)
dt=rpart(formula=Satisfaction~.,data=tr,control = r.ctrl)
plotcp(dt)
dt$cptable
r.ctrl=rpart.control(minisplit=50,minbucket=5,cp=0.00025,xval=10)
dt1=rpart(formula=Satisfaction~.,data=tr,control = r.ctrl)

ts$prob=predict(dt1,ts,type="prob")[,'1']
ts$pred=ifelse(ts$prob>0.5,1,0)
ts$pred=as.factor(ts$pred)
ts$Satisfaction=as.factor(ts$Satisfaction)
confusionMatrix(ts$Satisfaction,ts$pred,positive = "1")

```

## Random Forest
```{r}
library(randomForest)
#set seed again for randomness
set.seed(1000)
#build first RF model
rf=randomForest(Satisfaction~.,data=tr,ntree=200,mtry=3,nodesize=10,importance=T)
print(rf)
plot(rf)
#tune rf to identify the best mtry
set.seed(1000)
trrf=tuneRF(tr[,-c(1)],y=tr$Satisfaction,mtryStart = 3,stepFactor = 1.5,ntree=150,improve = 0.0001,nodesize=10,
            trace=T,plot=T,doBest = T,importance=T)
print(trrf)
plot(trrf)
rf1=randomForest(Satisfaction~.,data=tr,ntree=150,mtry=6,nodesize=10,importance=T)
plot(rf1)
print(rf1)

ts$prob=predict(rf1,ts,type="prob")[,'1']
ts$pred=ifelse(ts$prob>0.5,1,0)
ts$pred=as.factor(ts$pred)
ts$Satisfaction=as.factor(ts$Satisfaction)
confusionMatrix(ts$Satisfaction,ts$pred,positive = "1")
varImpPlot(rf1)
```

## XGboost Tuning
```{r}
library(caTools)
set.seed(5001)
spl=sample.split(sc$Satisfaction,SplitRatio = 0.7)
tr=subset(sc,spl==T)
ts=subset(sc,spl==F)
set.seed(1233)
library(xgboost)
gd_features_train<-as.matrix(tr[,-1])
gd_label_train<-as.matrix(tr[,1])
gd_features_test<-as.matrix(ts[,-1])
#checking for the best tuning parameters
tp_xgb<-vector()
lr=c(0.01,0.1,0.3,0.5,0.7,1)
md=c(1,3,5,9,7,15)
nr=c(50,100,500,1000)
mc=c(1,3,7,9,10,15)
gm=c(1,2,3,4,6,7,8,9,10)
for (i in mc){
xgb.fit <- xgboost(
  data = gd_features_train,
  label = gd_label_train,
  eta = 1,
  max_depth =15,
  min_child_weight = 1,
  nrounds = 100,
  nfold = 10,
  objective = "binary:logistic", 
  verbose = 0,               
  early_stopping_rounds = 10,
  gamma=1
)
tr$pred=predict(xgb.fit, gd_features_train)
tp_xgb=cbind(tp_xgb,sum(tr$Satisfaction==1 & tr$pred>0.5))
}
tp_xgb

```

## Tuned Xgboost
```{r}
library(xgboost)
library(caTools)
set.seed(502)
spl=sample.split(sc$Satisfaction,SplitRatio = 0.7)
tr=subset(sc,spl==T)
ts=subset(sc,spl==F)
set.seed(1243)
gd_features_train<-as.matrix(tr[,-1])
gd_label_train<-as.matrix(tr[,1])
gd_features_test<-as.matrix(ts[,-1])
xgb.fit <- xgboost(
  data = gd_features_train,
  label = gd_label_train,
  eta = 0.09,
  max_depth =15,
  min_child_weight = 1,
  nrounds = 100,
  nfold = 10,
  objective = "binary:logistic", 
  verbose = 0,               
  early_stopping_rounds = 10,
  
)

ts$pred=predict(xgb.fit, gd_features_test)

ts$prob=ifelse(ts$pred>0.5,1,0)
ts$prob=as.factor(ts$prob)


library(caret)
confusionMatrix(ts$prob,ts$Satisfaction,positive = "1")
xgb.importance(model=xgb.fit)
```

## Naive Bayes with 20 variables
```{r}
colSums(is.na(ff))
library(caret)
library(caTools)
ff$Satisfaction=as.factor(ff$Satisfaction)
nb=ff[,c(1:17,19,20)]
nb=lapply(nb, function(x) as.factor(x))
nb=as.data.frame(nb)
str(nb)
set.seed(502)
spl=sample.split(nb$Satisfaction,SplitRatio = 0.7)
tr=subset(nb,spl==T)
ts=subset(nb,spl==F)
set.seed(1243)
library(e1071)
nbb=naiveBayes(tr, tr$Satisfaction)
ts$pred=predict(nbb,ts,type="class")
confusionMatrix(ts$pred,ts$Satisfaction,positive = "1")
```

